{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sly in ./.venv/lib/python3.11/site-packages (0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Lexer\n",
    "from sly import Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalcLexer(Lexer):\n",
    "    # Set of token names. This is always required\n",
    "    tokens = {\n",
    "        'ID', 'EQUALS', 'LPAREN', 'RPAREN', 'COMMA', 'PLUS', 'DOT','LBRACKET', 'RBRACKET', 'TERNARY', 'NEW_LINE', 'COLON','AMPERSAND', 'EXCLAMATION', 'LBRACE', 'RBRACE'\n",
    "    }\n",
    "    \n",
    "    # String containing ignored characters between tokens\n",
    "    ignore = ' \\t'\n",
    "\n",
    "    COMMAND_ID = r'c\\d+'\n",
    "    OUTPUT_ID = r'y\\d+'\n",
    "    PARAMETER_Id=r'p\\d+'\n",
    "    ASYNC_OP_ID=r'a\\d+' \n",
    "    # Regular expression rules for tokens\n",
    "    ID      = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    EQUALS  = r'='\n",
    "    LPAREN  = r'\\('\n",
    "    RPAREN  = r'\\)'\n",
    "    COMMA   = r','\n",
    "    PLUS    = r'\\+'\n",
    "    DOT     = r'\\.'\n",
    "    LBRACKET = r'\\['\n",
    "    RBRACKET = r'\\]'\n",
    "    LBRACE = r'\\{'\n",
    "    RBRACE = r'\\}'\n",
    "    TERNARY  = r'\\?'\n",
    "    NEW_LINE = r'\\n'\n",
    "    COLON = r':'\n",
    "    AMPERSAND = r'&'\n",
    "    EXCLAMATION=r'!'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, type_, children=None, value=None):\n",
    "        self.type = type_\n",
    "        self.children = children\n",
    "        self.value = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&c1=userdata:get_content\n",
      "(c1(p1=i1)) + (c2(p1=i2,p8=o1.m1) +\n",
      "[c5(p6=e6)+c6(p7=e7)]o4.m2 +\n",
      "[c5(p6=e6)+c6(p7=e7)]o4.m3 +\n",
      "[c8(p1=i8)+c9(p3=i10)]{c1(p1=i3)}o4.m2 +\n",
      "c3(p1=i2,p8=i3) + c4(p1=i2,p8=i4))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token {TERNARY} defined, but not used\n",
      "WARNING: There is 1 unused token\n",
      "WARNING: 5 reduce/reduce conflicts\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "No symbol param_value. Must be one of {LBRACE, expression_list, RBRACE}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[275], line 194\u001b[0m\n\u001b[1;32m    187\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m&c1=userdata:get_content\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124m(c1(p1=i1)) + (c2(p1=i2,p8=o1.m1) +\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124m[c5(p6=e6)+c6(p7=e7)]o4.m2 +\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124m[c5(p6=e6)+c6(p7=e7)]o4.m3 +\u001b[39m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m[c8(p1=i8)+c9(p3=i10)]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mc1(p1=i3)}o4.m2 +\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124mc3(p1=i2,p8=i3) + c4(p1=i2,p8=i4))\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[0;32m--> 194\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tree_to_dict(result))\n",
      "File \u001b[0;32m~/chrome-ai-agent/.venv/lib/python3.11/site-packages/sly/yacc.py:2137\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   2135\u001b[0m sym \u001b[38;5;241m=\u001b[39m YaccSymbol()\n\u001b[1;32m   2136\u001b[0m sym\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m pname       \n\u001b[0;32m-> 2137\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpslice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m pslice:\n\u001b[1;32m   2139\u001b[0m     value \u001b[38;5;241m=\u001b[39m (pname, \u001b[38;5;241m*\u001b[39m(s\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pslice\u001b[38;5;241m.\u001b[39m_slice))\n",
      "Cell \u001b[0;32mIn[275], line 103\u001b[0m, in \u001b[0;36mCalcParser.block_producer\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;129m@_\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLBRACE expression_list RBRACE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblock_producer\u001b[39m(\u001b[38;5;28mself\u001b[39m,p):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlockProducer\u001b[39m\u001b[38;5;124m'\u001b[39m,value\u001b[38;5;241m=\u001b[39m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_value\u001b[49m, children\u001b[38;5;241m=\u001b[39m[p\u001b[38;5;241m.\u001b[39mblock])\n",
      "File \u001b[0;32m~/chrome-ai-agent/.venv/lib/python3.11/site-packages/sly/yacc.py:156\u001b[0m, in \u001b[0;36mYaccProduction.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     nameset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namemap) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo symbol \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnameset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: No symbol param_value. Must be one of {LBRACE, expression_list, RBRACE}."
     ]
    }
   ],
   "source": [
    "\n",
    "class CalcParser(Parser):\n",
    "    tokens = CalcLexer.tokens\n",
    "\n",
    "    precedence = (\n",
    "        ('left', 'PLUS'),\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        self.names = {}\n",
    "\n",
    "\n",
    "    @_('body')\n",
    "    def start(self, p):\n",
    "        return p.body\n",
    "    \n",
    "    @_('mapping_list NEW_LINE expression_list')\n",
    "    def body(self,p):\n",
    "        return Node('body', children=[p.mapping_list, p.expression_list])\n",
    "    \n",
    "    @_('mapping')\n",
    "    def mapping_list(self, p):\n",
    "        return Node('MappingList', children=[p.mapping])\n",
    "    \n",
    "    @_('mapping_list NEW_LINE mapping')\n",
    "    def mapping_list(self, p):\n",
    "        p.mapping_list.children.append(p.mapping)\n",
    "        return p.mapping_list\n",
    "\n",
    "    \n",
    "    @_('AMPERSAND ID EQUALS ID')\n",
    "    def mapping(self,p):\n",
    "        return Node('Mapping', value=[p.ID0,p.ID1])\n",
    "    \n",
    "    @_('AMPERSAND ID EQUALS ID COLON ID')\n",
    "    def mapping(self,p):\n",
    "        return Node('Mapping', value=[p.ID0,p.ID1,p.ID2])\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    @_('expression')\n",
    "    def expression_list(self, p):\n",
    "        return Node('ExpressionList', children=[p.expression])\n",
    "\n",
    "    @_('expression_list add expression')\n",
    "    def expression_list(self, p):\n",
    "        p.expression_list.children.append(p.expression)\n",
    "        return p.expression_list\n",
    "    \n",
    "    @_('yes_block_producer')\n",
    "    def expression(self, p):\n",
    "        return p.yes_block\n",
    "    \n",
    "    @_('no_block_producer')\n",
    "    def expression(self, p):\n",
    "        return p.no_block\n",
    "    \n",
    "    @_('yes_block')\n",
    "    def expression(self, p):\n",
    "        return p.yes_block\n",
    "    \n",
    "    @_('no_block')\n",
    "    def expression(self, p):\n",
    "        return p.no_block\n",
    "    \n",
    "    @_('block')\n",
    "    def expression(self, p):\n",
    "        return p.block\n",
    "\n",
    "    @_('parallel_expr')\n",
    "    def expression(self, p):\n",
    "        return p.parallel_expr\n",
    "\n",
    "    @_('PLUS')\n",
    "    def add(self, p):\n",
    "        return Node('plus')\n",
    "\n",
    "    # The plus_new_line rule simply includes the PLUS token\n",
    "    @_('PLUS NEW_LINE')\n",
    "    def add(self, p):\n",
    "        return Node('plus')\n",
    "    \n",
    "    @_('block block_producer block')\n",
    "    def yes_block_producer(self, p):\n",
    "        return Node('YesProducerBlock', value=p.block1, children=[p.block0, p.block_producer])\n",
    "    \n",
    "    @_('block block_producer block')\n",
    "    def no_block_producer(self, p):\n",
    "        return Node('NoProducerBlock', value=p.block1, children=[p.block0, p.block_producer])\n",
    "    \n",
    "    \n",
    "    @_('block block')\n",
    "    def conditonal_block(self, p):\n",
    "        return Node('YesBlock', value=p.block1, children=[p.block0])\n",
    "    \n",
    "    \n",
    "    @_('LBRACE RBRACE')\n",
    "    def block_producer(self,p):\n",
    "        return Node('BlockProducer', children=[])\n",
    "    \n",
    "    @_('LBRACE expression_list RBRACE')\n",
    "    def block_producer(self,p):\n",
    "        return Node('BlockProducer', children=[p.expression_list])\n",
    "    \n",
    "    @_('LBRACKET expression_list RBRACKET')\n",
    "    def block(self,p):\n",
    "        return Node('Block', children=[p.expression_list])\n",
    "    \n",
    "    @_('LBRACKET RBRACKET')\n",
    "    def block(self,p):\n",
    "        return Node('Block', children=[])\n",
    "    \n",
    "\n",
    "    @_('command')\n",
    "    def expression(self, p):\n",
    "        return p.command\n",
    "\n",
    "    @_('LPAREN expression_list RPAREN')\n",
    "    def parallel_expr(self, p):\n",
    "        return Node('ParallelBlock', children=[p.expression_list])\n",
    "    \n",
    "    @_('ID LPAREN param_list RPAREN')\n",
    "    def command(self, p):\n",
    "        return Node('Command', children=[Node('Id',value=p.ID), p.param_list])\n",
    "\n",
    "    @_('ID LPAREN RPAREN')\n",
    "    def command(self, p):\n",
    "        return Node('Command', children=[Node('Id',value=p.ID)])\n",
    "\n",
    "    @_('param')\n",
    "    def param_list(self, p):\n",
    "        return Node('ParamList',children=[p.param])\n",
    "\n",
    "    @_('param_list COMMA param')\n",
    "    def param_list(self, p):\n",
    "        p.param_list.children.append(p.param)\n",
    "        return p.param_list\n",
    "\n",
    "    @_('ID EQUALS ID')\n",
    "    def param(self, p):\n",
    "        return Node('Param', children=[Node('Id',value=p.ID0), Node('ParamValue', value=[p.ID1])])\n",
    "\n",
    "    @_('ID EQUALS param_value')\n",
    "    def param(self, p):\n",
    "        return Node('Param', children=[Node('Id',value=p.ID), p.param_value ])\n",
    "\n",
    "    @_('ID DOT ID')\n",
    "    def param_value(self, p):\n",
    "        return Node('ParamValue', value=[p.ID0,p.ID1])\n",
    "    \n",
    "    \n",
    "\n",
    "def tree_to_dict(node):\n",
    "    if not node:\n",
    "        return None\n",
    "\n",
    "    if node.children:\n",
    "        children_dict = [tree_to_dict(child) for child in node.children]\n",
    "    else:\n",
    "        children_dict = None\n",
    "\n",
    "    if isinstance(node.value, Node):\n",
    "        value_dict = tree_to_dict(node.value)\n",
    "    else:\n",
    "        value_dict = node.value\n",
    "\n",
    "    return {\n",
    "        'type': node.type,\n",
    "        'children': children_dict,\n",
    "        'value': value_dict\n",
    "    }\n",
    "\n",
    "def print_tree(node, indent=0):\n",
    "    if not node:\n",
    "        return\n",
    "    print('  ' * indent + str(node.type))\n",
    "    if node.children:\n",
    "        for child in node.children:\n",
    "            print_tree(child, indent + 1)\n",
    "    elif node.value is not None:\n",
    "        print('  ' * (indent + 1) + str(node.value))\n",
    "        if isinstance(node.value, Node):\n",
    "            print_tree(node.value, indent + 1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lexer = CalcLexer()\n",
    "    parser = CalcParser()\n",
    "    #CommandPlan\n",
    "    data = \"\"\"&c1=userdata:get_content\n",
    "(c1(p1=i1)) + (c2(p1=i2,p8=o1.m1) +\n",
    "[c5(p6=e6)+c6(p7=e7)]o4.m2 +\n",
    "[c5(p6=e6)+c6(p7=e7)]o4.m3 +\n",
    "[c8(p1=i8)+c9(p3=i10)]{c1(p1=i3)}o4.m2 +\n",
    "c3(p1=i2,p8=i3) + c4(p1=i2,p8=i4))\"\"\"\n",
    "    print(data)\n",
    "    result = parser.parse(lexer.tokenize(data))\n",
    "    print(tree_to_dict(result))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&c1=userdata:get_content\\n    &p1=file\\n    &sl1=sys:l\\n    &i1=crendentials\\n    &c2=userdata:append\\n    &p2=file\\n    &i2=backup_credentials\\n    &m1=content\\n    &m2=word_count\\n    &c3=userdata:append\\n    &p8=content\\n    &i3=Test\\n    &i4=Hello'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"&c1=userdata:get_content\n",
    "    &p1=file\n",
    "    &sl1=sys:l\n",
    "    &i1=crendentials\n",
    "    &c2=userdata:append\n",
    "    &p2=file\n",
    "    &i2=backup_credentials\n",
    "    &m1=content\n",
    "    &m2=word_count\n",
    "    &c3=userdata:append\n",
    "    &p8=content\n",
    "    &i3=Test\n",
    "    &i4=Hello\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABCLexer(Lexer):\n",
    "    # Set of token names. This is always required\n",
    "    tokens = {\n",
    "        'COMMAND_ID','OUTPUT_ID','PARAMETER_ID','ASYNC_OP_ID','RULE_ID','INPUT_ID',\n",
    "        'EQUALS', 'LPAREN', 'RPAREN', 'COMMA', 'PLUS', 'DOT','LBRACKET', 'RBRACKET', 'TERNARY', 'NEW_LINE', 'COLON','AMPERSAND', 'EXCLAMATION', 'LBRACE', 'RBRACE'\n",
    "    }\n",
    "    \n",
    "    # String containing ignored characters between tokens\n",
    "    ignore = ' \\t'\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    COMMAND_ID = r'c\\d+'\n",
    "    OUTPUT_ID = r'y\\d+'\n",
    "    PARAMETER_ID=r'p\\d+'\n",
    "    ASYNC_OP_ID=r'a\\d+'\n",
    "    RULE_ID=r'r\\d+'\n",
    "    INPUT_ID=r'x\\d+'\n",
    "     \n",
    "    # Regular expression rules for tokens\n",
    "    EQUALS  = r'='\n",
    "    LPAREN  = r'\\('\n",
    "    RPAREN  = r'\\)'\n",
    "    COMMA   = r','\n",
    "    PLUS    = r'\\+'\n",
    "    DOT     = r'\\.'\n",
    "    LBRACKET = r'\\['\n",
    "    RBRACKET = r'\\]'\n",
    "    LBRACE = r'\\{'\n",
    "    RBRACE = r'\\}'\n",
    "    TERNARY  = r'\\?'\n",
    "    NEW_LINE = r'\\n'\n",
    "    COLON = r':'\n",
    "    AMPERSAND = r'&'\n",
    "    EXCLAMATION=r'!'\n",
    "    \n",
    "    @_(r'\\n+')\n",
    "    def newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 - ai\n",
    "b1 - block memory\n",
    "\n",
    "\n",
    "h1([r1c1(){0 10 [\"sara\" \"mena\" ]}v1 + r2c1(){0 10 [\"sara\" \"mena\" ]}v2 + g1r2c2h1]a1){}  +  h2([r1p1{}])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Token(s) {RPAREN,LPAREN} defined, but not used\n",
      "WARNING: /var/folders/5f/y584tf1x3dj1h25jrtr_8l8c0000gn/T/ipykernel_13046/3031862520.py:25: Rule 'chars' defined, but not used\n",
      "WARNING: /var/folders/5f/y584tf1x3dj1h25jrtr_8l8c0000gn/T/ipykernel_13046/3031862520.py:29: Rule 'id' defined, but not used\n",
      "WARNING: There are 2 unused tokens\n",
      "WARNING: There are 2 unused rules\n",
      "WARNING: Symbol 'chars' is unreachable\n",
      "WARNING: Symbol 'id' is unreachable\n"
     ]
    },
    {
     "ename": "LexError",
     "evalue": "Illegal character '=' at index 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLexError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[287], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(p1=b1 p2=y1p2 p3=b3)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m tokens \u001b[38;5;241m=\u001b[39m lexer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[287], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(p1=b1 p2=y1p2 p3=b3)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m tokens \u001b[38;5;241m=\u001b[39m lexer\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/chrome-ai-agent/.venv/lib/python3.11/site-packages/sly/lex.py:444\u001b[0m, in \u001b[0;36mLexer.tokenize\u001b[0;34m(self, text, lineno, index)\u001b[0m\n\u001b[1;32m    442\u001b[0m tok\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    443\u001b[0m tok\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m text[index:]\n\u001b[0;32m--> 444\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     tok\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\n",
      "File \u001b[0;32m~/chrome-ai-agent/.venv/lib/python3.11/site-packages/sly/lex.py:460\u001b[0m, in \u001b[0;36mLexer.error\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LexError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIllegal character \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mvalue[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, t\u001b[38;5;241m.\u001b[39mvalue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[0;31mLexError\u001b[0m: Illegal character '=' at index 3"
     ]
    }
   ],
   "source": [
    "from sly import Lexer, Parser\n",
    "\n",
    "class MyLexer(Lexer):\n",
    "    tokens = {CHAR_SEQUENCE, NUMBER, PARAMETER, LPAREN, RPAREN}\n",
    "    ignore = ' \\t\\n'\n",
    "\n",
    "    CHAR_SEQUENCE = r'[a-zA-Z]+'\n",
    "    NUMBER = r'\\d+'\n",
    "    PARAMETER = r'\\([a-zA-Z]+\\=\\w+(?: [a-zA-Z]+\\=\\w+)*\\)'\n",
    "    LPAREN = r'\\('\n",
    "    RPAREN = r'\\)'\n",
    "\n",
    "\n",
    "class MyParser(Parser):\n",
    "    tokens = MyLexer.tokens\n",
    "\n",
    "    \n",
    "    @_('PARAMETER')\n",
    "    def statement(self, p):\n",
    "        parameter_str = p.PARAMETER[1:-1]  # Remove parentheses from the parameter string\n",
    "        parameters = parameter_str.split()  # Split the parameter string by spaces\n",
    "        parameter_list = [param.split('=') for param in parameters]  # Split each parameter by '='\n",
    "        return f\"PARAMETERS: {parameter_list}\"\n",
    "    \n",
    "    @_('CHAR_SEQUENCE')\n",
    "    def chars(self, p):\n",
    "        return f\"CHAR_SEQUENCE: {p.CHAR_SEQUENCE}\"\n",
    "\n",
    "    @_('NUMBER')\n",
    "    def id(self, p):\n",
    "        return f\"NUMBER: {p.NUMBER}\"\n",
    "\n",
    "lexer = MyLexer()\n",
    "parser = MyParser()\n",
    "\n",
    "text = \"r1c10(p1=b1 p2=y1p2 p3=b3)\"\n",
    "tokens = lexer.tokenize(text)\n",
    "print([val for val in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
